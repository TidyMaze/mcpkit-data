# ğŸš€ mcpkit-data

> **The ultimate MCP server for data engineers who want to stop context-switching and start shipping** âœ¨

A comprehensive [Model Context Protocol](https://modelcontextprotocol.io) server that gives your AI assistant superpowers to work with Kafka, databases, AWS, data pipelines, and more. Stop jumping between terminals, dashboards, and docs. Just ask your AI to do it.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## âš ï¸ Early Days Disclaimer

**Hey there! ğŸ‘‹** Before you dive in, here's the deal:

This is a **brand new project** fresh out of the oven ğŸ. We're talking:
- ğŸ†• **First release** - Like, literally just born
- ğŸ§ª **Not production-ready** - More like "works on my machine" ready
- ğŸ‘¥ **Limited testing** - It's mostly been tested by... well, me
- ğŸ› ï¸ **Actively evolving** - Things might break, change, or surprise you

**What this means:**
- âœ… Great for experimentation and learning
- âœ… Perfect for side projects and personal use
- âœ… Awesome if you want to contribute and shape the future
- âŒ Probably not ideal for critical production systems (yet!)
- âŒ Might have bugs, quirks, or "interesting" behaviors
- âŒ Documentation might be incomplete or confusing

**But here's the cool part:** You can help make it better! ğŸš€ Found a bug? Report it! Want a feature? Ask for it! Want to contribute? We'd love that!

*TL;DR: This is the "early access" version. Use at your own risk, have fun, and help us make it awesome!* ğŸ‰

---

## ğŸ¯ What is this?

**mcpkit-data** is your AI assistant's Swiss Army knife for data engineering. It exposes **66+ tools** through the MCP protocol, letting your AI:

- ğŸ” **Debug Kafka pipelines** without leaving your editor
- ğŸ“Š **Query databases** (JDBC, Athena) and analyze results
- ğŸ¼ **Process data** with pandas, polars, and DuckDB
- â˜ï¸ **Work with AWS** (Athena, S3, Glue) seamlessly
- ğŸ—ï¸ **Inspect infrastructure** (Nomad, Consul) in real-time
- ğŸ“ˆ **Generate charts** and evidence bundles automatically
- âœ… **Validate data quality** with Great Expectations

**All without you writing a single line of code.** Just chat with your AI. ğŸ‰

---

## âš¡ Quick Start

### Installation

```bash
pip install -e ".[dev]"
```

### Run the Server

```bash
python -m mcpkit.server
```

### Configure Cursor (or your MCP client)

Add to your Cursor MCP settings:

```json
{
  "mcpServers": {
    "mcpkit-data": {
      "command": "python",
      "args": ["-m", "mcpkit.server"],
      "env": {
        "MCPKIT_KAFKA_BOOTSTRAP": "localhost:9092",
        "MCPKIT_SCHEMA_REGISTRY_URL": "http://localhost:8081",
        "MCPKIT_JDBC_DRIVER_CLASS": "org.postgresql.Driver",
        "MCPKIT_JDBC_URL": "jdbc:postgresql://localhost/test",
        "MCPKIT_JDBC_JARS": "/path/to/postgresql.jar",
        "MCPKIT_ROOTS": "/allowed/path1:/allowed/path2"
      }
    }
  }
}
```

**That's it!** Your AI can now use all 66+ tools. ğŸŠ

---

## ğŸ› ï¸ Tools Overview

### ğŸ“¦ Dataset Registry (4 tools)

Your data's home base. Store, query, and manage datasets as Parquet files.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `dataset_list` | ğŸ“‹ List all datasets | See what data you have |
| `dataset_info` | â„¹ï¸ Get dataset metadata | Check schema, row count |
| `dataset_put_rows` | ğŸ’¾ Store rows as Parquet | Save query results |
| `dataset_delete` | ğŸ—‘ï¸ Delete a dataset | Clean up old data |

---

### ğŸ§ Kafka Tools (8 tools)

Debug, consume, and analyze Kafka topics like a pro.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `kafka_list_topics` | ğŸ“ List all topics | Discover what's available |
| `kafka_offsets` | ğŸ“Š Get partition offsets | Check lag, find positions |
| `kafka_consume_batch` | ğŸ“¥ Consume messages â†’ dataset | Extract data for analysis |
| `kafka_consume_tail` | ğŸ”š Get last N messages | Debug recent events |
| `kafka_filter` | ğŸ” Filter with JMESPath | Find specific records |
| `kafka_flatten` | ğŸ“ Flatten nested records | Normalize JSON structures |
| `kafka_groupby_key` | ğŸ”‘ Group by extracted key | Aggregate by message key |
| `kafka_describe_topic` | ğŸ“‹ Get topic config/partitions | Inspect topic metadata |

---

### ğŸ“‹ Schema Registry & Decoding (4 tools)

Work with Avro, Protobuf, and schema discovery.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `schema_registry_get` | ğŸ“„ Get schema by ID/subject | Fetch schema definitions |
| `schema_registry_list_subjects` | ğŸ“š List all subjects | Discover available schemas |
| `avro_decode` | ğŸ”“ Decode Avro messages | Parse binary Avro data |
| `protobuf_decode` | ğŸ”“ Decode Protobuf messages | Parse binary Protobuf data |

---

### ğŸ—„ï¸ Database Tools (2 tools)

Query databases safely with read-only enforcement.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `jdbc_query_ro` | ğŸ” Execute read-only SQL | Query any JDBC database |
| `jdbc_introspect` | ğŸ” Introspect schema | Discover tables/columns |

**Safety**: Automatically blocks `DROP`, `INSERT`, `UPDATE`, `DELETE`, etc. âœ…

---

### â˜ï¸ AWS Tools (9 tools)

Query Athena, list S3, manage Glue partitions.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `athena_start_query` | ğŸš€ Start SQL query | Run analytics queries |
| `athena_poll_query` | â³ Check query status | Monitor execution |
| `athena_get_results` | ğŸ“Š Get query results | Fetch data |
| `athena_explain` | ğŸ“– Explain query plan | Optimize queries |
| `athena_partitions_list` | ğŸ“‚ List partitions | Check table structure |
| `athena_repair_table` | ğŸ”§ Repair table (MSCK) | Fix partition metadata |
| `athena_ctas_export` | ğŸ“¤ CTAS export | Export to S3 |
| `s3_list_prefix` | ğŸ“ List S3 objects | Browse buckets |

**Safety**: SQL validation blocks dangerous operations by default. âœ…

---

### ğŸ¼ Pandas Tools (12 tools)

The data scientist's best friend, now in your AI assistant.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `pandas_from_rows` | ğŸ“Š Create DataFrame | Build datasets |
| `pandas_describe` | ğŸ“ˆ Get statistics | Understand your data |
| `pandas_groupby` | ğŸ”¢ Group + aggregate | Summarize by categories |
| `pandas_join` | ğŸ”— Join datasets | Combine data sources |
| `pandas_filter_query` | ğŸ” Filter rows | Subset your data |
| `pandas_filter_time_range` | â° Filter by time | Time-series analysis |
| `pandas_diff_frames` | ğŸ”„ Compare datasets | Find differences |
| `pandas_schema_check` | âœ… Validate schema | Ensure data quality |
| `pandas_sample_stratified` | ğŸ² Stratified sample | Balanced sampling |
| `pandas_sample_random` | ğŸ² Random sample | Quick data preview |
| `pandas_count_distinct` | ğŸ”¢ Count unique values | Cardinality analysis |
| `pandas_export` | ğŸ’¾ Export to CSV/Parquet/Excel | Save results |
| `dataset_head_tail` | ğŸ“„ Get first/last N rows | Quick data preview |

---

### âš¡ Polars Tools (3 tools)

Lightning-fast data processing with Polars.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `polars_from_rows` | ğŸ“Š Create DataFrame | Build datasets |
| `polars_groupby` | ğŸ”¢ Group + aggregate | Fast aggregations |
| `polars_export` | ğŸ’¾ Export to CSV/Parquet/JSON | Save results |

---

### ğŸ¦† DuckDB Tools (1 tool)

SQL on local data sources. No server needed.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `duckdb_query_local` | ğŸ” SQL on local data | Query Parquet/CSV files |

---

### ğŸ¨ JSON & Data Quality (5 tools)

Transform, validate, and correlate events.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `jq_transform` | ğŸ”„ JMESPath transform | Extract/transform JSON |
| `event_validate` | âœ… JSONSchema validation | Validate data structure |
| `event_fingerprint` | ğŸ”‘ Generate fingerprint | Create unique IDs |
| `dedupe_by_id` | ğŸ§¹ Deduplicate records | Remove duplicates |
| `event_correlate` | ğŸ”— Correlate events | Join across batches |

---

### ğŸ“ File & Format Tools (4 tools)

Work with files and convert formats.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `parquet_inspect` | ğŸ” Inspect Parquet schema | Understand file structure |
| `arrow_convert` | ğŸ”„ Convert formats | Parquet â†” CSV â†” JSON |
| `fs_read` | ğŸ“– Read file (with offset) | Read large files efficiently |
| `fs_list_dir` | ğŸ“‚ List directory | Browse filesystem |
| `rg_search` | ğŸ” Search with ripgrep | Find patterns in code/data |

---

### ğŸ“Š Visualization & Evidence (2 tools)

Generate charts and evidence bundles automatically.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `dataset_to_chart` | ğŸ“ˆ Auto-generate charts | Visualize data instantly |
| `evidence_bundle_plus` | ğŸ“¦ Generate evidence bundle | Export stats + samples |

---

### ğŸ—ï¸ Infrastructure Tools (8 tools)

Monitor Nomad jobs and Consul services.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `nomad_list_jobs` | ğŸ“‹ List Nomad jobs (fuzzy search) | Find running services |
| `nomad_get_job_status` | ğŸ“Š Get full job status | Inspect job details |
| `nomad_get_allocation_logs` | ğŸ“œ Read allocation logs | Debug failing jobs |
| `nomad_get_allocation_events` | ğŸ“… Get lifecycle events | Track job history |
| `nomad_get_node_status` | ğŸ–¥ï¸ Get node status | Check node health |
| `consul_get_service_ips` | ğŸŒ Get service IPs (fuzzy) | Find service endpoints |
| `consul_list_services` | ğŸ“š List all services | Discover service catalog |
| `consul_get_service_health` | â¤ï¸ Get service health | Check service status |

**Fuzzy search**: Find services even with partial names! ğŸ¯

---

### ğŸŒ HTTP Tools (1 tool)

Make HTTP requests safely.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `http_request` | ğŸŒ HTTP GET/POST | Call APIs, test endpoints |

**Safety**: POST disabled by default. Opt-in required. âœ…

---

### âœ… Great Expectations (1 tool)

Data quality validation with GE.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `great_expectations_check` | âœ… Run GE validation | Validate data quality |

---

### ğŸ”„ Reconciliation (1 tool)

Compare datasets and find differences.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `reconcile_counts` | ğŸ” Reconcile record counts | Compare datasets |

---

## ğŸ”’ Security & Guardrails

### ğŸ›¡ï¸ Built-in Safety

- **Filesystem Allowlist**: All file operations restricted to `MCPKIT_ROOTS`
- **SQL Validation**: JDBC and Athena queries validated (read-only by default)
- **HTTP Safety**: POST requests disabled by default
- **Output Limits**: Automatic caps on rows, records, and bytes
- **Path Traversal Protection**: Blocks `..` and unsafe paths

### ğŸ“ Configurable Limits

| Variable | Default | What it does |
|----------|---------|--------------|
| `MCPKIT_TIMEOUT_SECS` | `15` | Operation timeout |
| `MCPKIT_MAX_OUTPUT_BYTES` | `1000000` | Max output size |
| `MCPKIT_MAX_ROWS` | `500` | Max rows returned |
| `MCPKIT_MAX_RECORDS` | `500` | Max records returned |

---

## âš™ï¸ Configuration

### ğŸ” AWS Credentials

AWS uses the standard boto3 credential chain:

1. **Environment Variables** (recommended):
   ```bash
   export AWS_ACCESS_KEY_ID=your-key
   export AWS_SECRET_ACCESS_KEY=your-secret
   export AWS_REGION=eu-central-1
   ```

2. **AWS Credentials File** (`~/.aws/credentials`):
   ```ini
   [default]
   aws_access_key_id = your-key
   aws_secret_access_key = your-secret
   ```

3. **IAM Roles** (auto-detected on EC2/ECS/Lambda)

4. **AWS SSO** (use `aws sso login` first)

### ğŸ“ Filesystem Security

```bash
export MCPKIT_ROOTS="/allowed/path1:/allowed/path2"
```

### ğŸ§ Kafka Configuration

```bash
export MCPKIT_KAFKA_BOOTSTRAP="localhost:9092"
export MCPKIT_KAFKA_SECURITY_PROTOCOL="SASL_SSL"  # optional
export MCPKIT_KAFKA_SASL_MECHANISM="PLAIN"  # optional
export MCPKIT_KAFKA_SASL_USERNAME="user"  # optional
export MCPKIT_KAFKA_SASL_PASSWORD="pass"  # optional
```

### ğŸ“‹ Schema Registry

```bash
export MCPKIT_SCHEMA_REGISTRY_URL="http://localhost:8081"
export MCPKIT_SCHEMA_REGISTRY_BASIC_AUTH="user:pass"  # optional
```

### ğŸ—„ï¸ JDBC

```bash
export MCPKIT_JDBC_DRIVER_CLASS="org.postgresql.Driver"
export MCPKIT_JDBC_URL="jdbc:postgresql://localhost/test"
export MCPKIT_JDBC_JARS="/path/to/postgresql.jar"
export MCPKIT_JDBC_USER="user"  # optional
export MCPKIT_JDBC_PASSWORD="pass"  # optional
```

---

## ğŸ¯ Example Workflows

### ğŸ” End-to-End Kafka Pipeline Debugging

**You:** "Debug the 'order-events' topic: consume recent messages, decode Avro, filter errors, analyze patterns, and create a chart"

**AI Response:**
```
I'll debug the order-events pipeline end-to-end.

[Step 1: kafka_consume_tail] Consuming last 50 messages...
âœ“ Consumed 50 messages from 'order-events'

[Step 2: schema_registry_get] Fetching Avro schema...
âœ“ Schema retrieved: order-event v2.3 (ID: 145)

[Step 3: avro_decode] Decoding Avro messages...
âœ“ Decoded 50 messages successfully

[Step 4: kafka_filter] Filtering error events...
âœ“ Found 8 error events matching criteria

[Step 5: pandas_groupby] Analyzing error patterns...
Error breakdown:
- Payment failures: 4 (50%)
- Validation errors: 2 (25%)
- Timeout errors: 2 (25%)

[Step 6: dataset_to_chart] Creating visualization...
âœ“ Chart saved: .artifacts/order_errors_analysis.png

[Step 7: pandas_export] Exporting error details...
âœ“ CSV exported: .artifacts/order_errors_20251221.csv

Summary: 8 errors out of 50 messages (16% error rate). Payment failures are the primary issue.
```

**Tools used:** `kafka_consume_tail` â†’ `schema_registry_get` â†’ `avro_decode` â†’ `kafka_filter` â†’ `pandas_groupby` â†’ `dataset_to_chart` â†’ `pandas_export` (7 tools)

---

### ğŸ“Š Multi-Source Data Reconciliation

**You:** "Compare sales data between Athena and Postgres: query both, join on order_id, find discrepancies, and export differences"

**AI Response:**
```
I'll reconcile sales data between Athena and Postgres.

[Step 1: athena_start_query] Querying Athena sales table...
âœ“ Query started: execution_id abc123

[Step 2: athena_poll_query] Waiting for completion...
âœ“ Query completed in 3.2s (scanned 1.2 GB)

[Step 3: athena_get_results] Fetching results...
âœ“ Retrieved 5,234 rows from Athena

[Step 4: pandas_from_rows] Storing Athena data...
âœ“ Dataset created: athena_sales_20251221

[Step 5: jdbc_query_ro] Querying Postgres sales table...
âœ“ Retrieved 5,189 rows from Postgres

[Step 6: pandas_from_rows] Storing Postgres data...
âœ“ Dataset created: postgres_sales_20251221

[Step 7: pandas_join] Joining datasets on order_id...
âœ“ Joined datasets: 5,180 matching orders

[Step 8: pandas_diff_frames] Finding discrepancies...
Discrepancies found:
- Missing in Postgres: 54 orders
- Missing in Athena: 9 orders
- Amount mismatches: 12 orders
- Total discrepancy value: $3,456.78

[Step 9: pandas_export] Exporting differences...
âœ“ Differences exported: .artifacts/sales_reconciliation_diff.csv

[Step 10: reconcile_counts] Generating reconciliation report...
âœ“ Report saved: .artifacts/sales_reconciliation_report.json
```

**Tools used:** `athena_start_query` â†’ `athena_poll_query` â†’ `athena_get_results` â†’ `pandas_from_rows` â†’ `jdbc_query_ro` â†’ `pandas_from_rows` â†’ `pandas_join` â†’ `pandas_diff_frames` â†’ `pandas_export` â†’ `reconcile_counts` (10 tools)

---

### ğŸ—ï¸ Service Health Investigation

**You:** "Investigate the 'payment-service': find it in Nomad, check Consul health, get logs, filter errors, correlate with events, and export report"

**AI Response:**
```
I'll investigate the payment-service health comprehensively.

[Step 1: nomad_list_jobs] Finding payment-service...
âœ“ Found: platform-payment-service-app (status: running)

[Step 2: nomad_get_job_status] Getting job details...
âœ“ Job running on node: node-abc-123
âœ“ Allocation: alloc-def-456

[Step 3: consul_get_service_health] Checking Consul health...
âœ“ Service health: passing (3/3 checks passing)
âœ“ Service IPs: 10.0.1.45:8080, 10.0.1.46:8080

[Step 4: nomad_get_allocation_logs] Retrieving logs...
âœ“ Retrieved last 200 log lines

[Step 5: pandas_from_rows] Storing logs as dataset...
âœ“ Dataset created: payment_service_logs

[Step 6: pandas_filter_query] Filtering error logs...
âœ“ Found 15 error entries in last hour

[Step 7: kafka_consume_batch] Consuming related events...
âœ“ Consumed 100 payment events from Kafka

[Step 8: event_correlate] Correlating logs with events...
âœ“ Correlated 12 error logs with payment events

[Step 9: pandas_groupby] Analyzing error patterns...
Error analysis:
- Database timeouts: 8 (53%)
- External API failures: 4 (27%)
- Validation errors: 3 (20%)

[Step 10: evidence_bundle_plus] Generating evidence bundle...
âœ“ Bundle created:
  - .artifacts/payment_service_investigation_stats.json
  - .artifacts/payment_service_error_logs.csv
  - .artifacts/payment_service_correlated_events.csv

Summary: Service is healthy but experiencing intermittent database timeouts affecting 8 transactions.
```

**Tools used:** `nomad_list_jobs` â†’ `nomad_get_job_status` â†’ `consul_get_service_health` â†’ `nomad_get_allocation_logs` â†’ `pandas_from_rows` â†’ `pandas_filter_query` â†’ `kafka_consume_batch` â†’ `event_correlate` â†’ `pandas_groupby` â†’ `evidence_bundle_plus` (10 tools)

---

### âœ… Data Quality Validation Pipeline

**You:** "Validate Kafka events: consume batch, flatten nested JSON, validate against schema, check data quality, and generate report"

**AI Response:**
```
I'll run a complete data quality validation on Kafka events.

[Step 1: kafka_consume_batch] Consuming event batch...
âœ“ Consumed 500 messages from 'user-actions' topic

[Step 2: kafka_flatten] Flattening nested JSON structures...
âœ“ Flattened 500 records into 45 columns
âœ“ Dataset created: user_actions_flattened

[Step 3: schema_registry_get] Fetching validation schema...
âœ“ Retrieved JSONSchema: user-action-event v1.2

[Step 4: pandas_sample_random] Sampling for validation...
âœ“ Sampled 50 random records

[Step 5: event_validate] Validating against schema...
Validation results:
- Valid records: 47 (94%)
- Invalid records: 3 (6%)
  - Missing required field 'timestamp': 2
  - Invalid enum value 'action_type': 1

[Step 6: pandas_schema_check] Checking data types and constraints...
âœ“ Schema check passed: all required columns present
âš ï¸ Warning: 12% null values in 'metadata' column

[Step 7: pandas_count_distinct] Analyzing data quality...
Distinct value counts:
- user_id: 234 unique (expected: ~250)
- action_type: 8 distinct (expected: 10)
- session_id: 198 unique

[Step 8: pandas_filter_time_range] Filtering recent events...
âœ“ Filtered to last 24 hours: 487 events

[Step 9: great_expectations_check] Running GE validation suite...
âœ“ Expectations passed: 8/10
âš ï¸ Failed: 
  - expect_column_values_to_be_unique (user_id)
  - expect_column_values_to_not_be_null (metadata)

[Step 10: evidence_bundle_plus] Generating quality report...
âœ“ Quality report: .artifacts/data_quality_report_20251221.zip
```

**Tools used:** `kafka_consume_batch` â†’ `kafka_flatten` â†’ `schema_registry_get` â†’ `pandas_sample_random` â†’ `event_validate` â†’ `pandas_schema_check` â†’ `pandas_count_distinct` â†’ `pandas_filter_time_range` â†’ `great_expectations_check` â†’ `evidence_bundle_plus` (10 tools)

---

### ğŸ“ˆ Time-Series Analysis & Forecasting

**You:** "Analyze sales trends: query Athena for last 6 months, filter by date range, group by week, sample for quality, create chart, and export"

**AI Response:**
```
I'll analyze sales trends over the last 6 months.

[Step 1: athena_start_query] Starting sales query...
âœ“ Query: SELECT * FROM sales WHERE date >= '2025-06-01'

[Step 2: athena_poll_query] Monitoring execution...
âœ“ Query running... (scanned 2.1 GB so far)

[Step 3: athena_get_results] Fetching results...
âœ“ Retrieved 45,678 sales records

[Step 4: pandas_from_rows] Storing in dataset...
âœ“ Dataset created: sales_6months

[Step 5: pandas_filter_time_range] Filtering date range...
âœ“ Filtered: 2025-06-01 to 2025-12-21 (45,234 records)

[Step 6: pandas_groupby] Grouping by week and region...
âœ“ Aggregated: 26 weeks Ã— 4 regions = 104 groups

[Step 7: pandas_sample_stratified] Stratified sampling...
âœ“ Sampled 20 records per region for validation

[Step 8: pandas_describe] Statistical analysis...
Statistics:
- Total sales: $12,345,678
- Average per week: $474,833
- Trend: +5.2% week-over-week
- Peak week: Week 24 ($567,890)

[Step 9: dataset_to_chart] Creating time-series chart...
âœ“ Chart: .artifacts/sales_trends_6months.png

[Step 10: pandas_export] Exporting aggregated data...
âœ“ Exported: .artifacts/sales_by_week_region.csv (104 rows)
âœ“ Exported: .artifacts/sales_statistics.json
```

**Tools used:** `athena_start_query` â†’ `athena_poll_query` â†’ `athena_get_results` â†’ `pandas_from_rows` â†’ `pandas_filter_time_range` â†’ `pandas_groupby` â†’ `pandas_sample_stratified` â†’ `pandas_describe` â†’ `dataset_to_chart` â†’ `pandas_export` (10 tools)

---

### ğŸ”„ Cross-System Event Correlation

**You:** "Correlate user events across systems: get Kafka events, query database for user profiles, join datasets, deduplicate, correlate timestamps, and export"

**AI Response:**
```
I'll correlate user events across Kafka and database systems.

[Step 1: kafka_consume_batch] Consuming user events...
âœ“ Consumed 1,000 events from 'user-events' topic

[Step 2: kafka_flatten] Flattening event structure...
âœ“ Flattened to 1,000 records with 32 columns

[Step 3: jdbc_query_ro] Querying user profiles...
âœ“ Retrieved 850 user profiles from database

[Step 4: pandas_from_rows] Storing profiles...
âœ“ Dataset created: user_profiles

[Step 5: pandas_join] Joining events with profiles...
âœ“ Joined on user_id: 987 matched records

[Step 6: dedupe_by_id] Removing duplicate events...
âœ“ Deduplicated: 23 duplicates removed (964 unique)

[Step 7: event_correlate] Correlating by timestamp...
âœ“ Correlated events into 234 user sessions

[Step 8: pandas_groupby] Analyzing session patterns...
Session analysis:
- Average session duration: 12.5 minutes
- Events per session: 4.1
- Most active users: 12 users with 10+ events

[Step 9: pandas_filter_query] Filtering high-value sessions...
âœ“ Found 45 sessions with purchase events

[Step 10: pandas_export] Exporting correlated data...
âœ“ Exported: .artifacts/correlated_user_sessions.csv
âœ“ Exported: .artifacts/session_analysis.json
```

**Tools used:** `kafka_consume_batch` â†’ `kafka_flatten` â†’ `jdbc_query_ro` â†’ `pandas_from_rows` â†’ `pandas_join` â†’ `dedupe_by_id` â†’ `event_correlate` â†’ `pandas_groupby` â†’ `pandas_filter_query` â†’ `pandas_export` (10 tools)

---

## ğŸ§ª Testing

```bash
pytest
```

All tests use mocks/stubs. No real connections needed! âœ…

---

## ğŸ¤ Contributing

We â¤ï¸ contributions! Here's how to help:

1. **ğŸ› Found a bug?** Open an issue
2. **ğŸ’¡ Have an idea?** Propose a new tool or feature
3. **ğŸ”§ Want to code?** Check out `TOOLS_ANALYSIS.md` for gaps
4. **ğŸ“ Docs unclear?** Improve them!

**Design Principles:**
- ğŸ¯ **KISS**: One tool, one job
- ğŸ”— **Composable**: Tools work together seamlessly
- ğŸ”’ **Safe**: Read-only by default, explicit opt-ins
- âš¡ **Fast**: Efficient operations, smart limits

---

## ğŸ“š Learn More

- [Model Context Protocol](https://modelcontextprotocol.io) - The protocol we use
- [FastMCP](https://github.com/jlowin/fastmcp) - The framework we build on
- [Cursor](https://cursor.sh) - The IDE that makes this shine âœ¨

---

## ğŸ“„ License

MIT License - Use it, fork it, make it better! ğŸš€

---

**Made with â¤ï¸ for data engineers who want to ship faster**

*Stop context-switching. Start shipping.* ğŸ¯
