# ğŸš€ mcpkit-data

> **The ultimate MCP server for data engineers who want to stop context-switching and start shipping** âœ¨

A comprehensive [Model Context Protocol](https://modelcontextprotocol.io) server that gives your AI assistant superpowers to work with Kafka, databases, AWS, data pipelines, and more. Stop jumping between terminals, dashboards, and docs. Just ask your AI to do it.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## âš ï¸ Early Days Disclaimer

**Hey there! ğŸ‘‹** Before you dive in, here's the deal:

This is a **brand new project** fresh out of the oven ğŸ. We're talking:
- ğŸ†• **First release** - Like, literally just born
- ğŸ§ª **Not production-ready** - More like "works on my machine" ready
- ğŸ‘¥ **Limited testing** - It's mostly been tested by... well, me
- ğŸ› ï¸ **Actively evolving** - Things might break, change, or surprise you

**What this means:**
- âœ… Great for experimentation and learning
- âœ… Perfect for side projects and personal use
- âœ… Awesome if you want to contribute and shape the future
- âŒ Probably not ideal for critical production systems (yet!)
- âŒ Might have bugs, quirks, or "interesting" behaviors
- âŒ Documentation might be incomplete or confusing

**But here's the cool part:** You can help make it better! ğŸš€ Found a bug? Report it! Want a feature? Ask for it! Want to contribute? We'd love that!

*TL;DR: This is the "early access" version. Use at your own risk, have fun, and help us make it awesome!* ğŸ‰

---

## ğŸ¯ What is this?

**mcpkit-data** is your AI assistant's Swiss Army knife for data engineering. It exposes **66+ tools** through the MCP protocol, letting your AI:

- ğŸ” **Debug Kafka pipelines** without leaving your editor
- ğŸ“Š **Query databases** (JDBC, Athena) and analyze results
- ğŸ¼ **Process data** with pandas, polars, and DuckDB
- â˜ï¸ **Work with AWS** (Athena, S3, Glue) seamlessly
- ğŸ—ï¸ **Inspect infrastructure** (Nomad, Consul) in real-time
- ğŸ“ˆ **Generate charts** and evidence bundles automatically
- âœ… **Validate data quality** with Great Expectations

**All without you writing a single line of code.** Just chat with your AI. ğŸ‰

---

## âš¡ Quick Start

### Installation

```bash
pip install -e ".[dev]"
```

### Run the Server

```bash
python -m mcpkit.server
```

### Configure Cursor (or your MCP client)

Add to your Cursor MCP settings:

```json
{
  "mcpServers": {
    "mcpkit-data": {
      "command": "python",
      "args": ["-m", "mcpkit.server"],
      "env": {
        "MCPKIT_KAFKA_BOOTSTRAP": "localhost:9092",
        "MCPKIT_SCHEMA_REGISTRY_URL": "http://localhost:8081",
        "MCPKIT_JDBC_DRIVER_CLASS": "org.postgresql.Driver",
        "MCPKIT_JDBC_URL": "jdbc:postgresql://localhost/test",
        "MCPKIT_JDBC_JARS": "/path/to/postgresql.jar",
        "MCPKIT_ROOTS": "/allowed/path1:/allowed/path2"
      }
    }
  }
}
```

**That's it!** Your AI can now use all 66+ tools. ğŸŠ

---

## ğŸ› ï¸ Tools Overview

### ğŸ“¦ Dataset Registry (4 tools)

Your data's home base. Store, query, and manage datasets as Parquet files.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `dataset_list` | ğŸ“‹ List all datasets | See what data you have |
| `dataset_info` | â„¹ï¸ Get dataset metadata | Check schema, row count |
| `dataset_put_rows` | ğŸ’¾ Store rows as Parquet | Save query results |
| `dataset_delete` | ğŸ—‘ï¸ Delete a dataset | Clean up old data |

---

### ğŸ§ Kafka Tools (8 tools)

Debug, consume, and analyze Kafka topics like a pro.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `kafka_list_topics` | ğŸ“ List all topics | Discover what's available |
| `kafka_offsets` | ğŸ“Š Get partition offsets | Check lag, find positions |
| `kafka_consume_batch` | ğŸ“¥ Consume messages â†’ dataset | Extract data for analysis |
| `kafka_consume_tail` | ğŸ”š Get last N messages | Debug recent events |
| `kafka_filter` | ğŸ” Filter with JMESPath | Find specific records |
| `kafka_flatten` | ğŸ“ Flatten nested records | Normalize JSON structures |
| `kafka_groupby_key` | ğŸ”‘ Group by extracted key | Aggregate by message key |
| `kafka_describe_topic` | ğŸ“‹ Get topic config/partitions | Inspect topic metadata |

---

### ğŸ“‹ Schema Registry & Decoding (4 tools)

Work with Avro, Protobuf, and schema discovery.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `schema_registry_get` | ğŸ“„ Get schema by ID/subject | Fetch schema definitions |
| `schema_registry_list_subjects` | ğŸ“š List all subjects | Discover available schemas |
| `avro_decode` | ğŸ”“ Decode Avro messages | Parse binary Avro data |
| `protobuf_decode` | ğŸ”“ Decode Protobuf messages | Parse binary Protobuf data |

---

### ğŸ—„ï¸ Database Tools (2 tools)

Query databases safely with read-only enforcement.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `jdbc_query_ro` | ğŸ” Execute read-only SQL | Query any JDBC database |
| `jdbc_introspect` | ğŸ” Introspect schema | Discover tables/columns |

**Safety**: Automatically blocks `DROP`, `INSERT`, `UPDATE`, `DELETE`, etc. âœ…

---

### â˜ï¸ AWS Tools (9 tools)

Query Athena, list S3, manage Glue partitions.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `athena_start_query` | ğŸš€ Start SQL query | Run analytics queries |
| `athena_poll_query` | â³ Check query status | Monitor execution |
| `athena_get_results` | ğŸ“Š Get query results | Fetch data |
| `athena_explain` | ğŸ“– Explain query plan | Optimize queries |
| `athena_partitions_list` | ğŸ“‚ List partitions | Check table structure |
| `athena_repair_table` | ğŸ”§ Repair table (MSCK) | Fix partition metadata |
| `athena_ctas_export` | ğŸ“¤ CTAS export | Export to S3 |
| `s3_list_prefix` | ğŸ“ List S3 objects | Browse buckets |

**Safety**: SQL validation blocks dangerous operations by default. âœ…

---

### ğŸ¼ Pandas Tools (12 tools)

The data scientist's best friend, now in your AI assistant.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `pandas_from_rows` | ğŸ“Š Create DataFrame | Build datasets |
| `pandas_describe` | ğŸ“ˆ Get statistics | Understand your data |
| `pandas_groupby` | ğŸ”¢ Group + aggregate | Summarize by categories |
| `pandas_join` | ğŸ”— Join datasets | Combine data sources |
| `pandas_filter_query` | ğŸ” Filter rows | Subset your data |
| `pandas_filter_time_range` | â° Filter by time | Time-series analysis |
| `pandas_diff_frames` | ğŸ”„ Compare datasets | Find differences |
| `pandas_schema_check` | âœ… Validate schema | Ensure data quality |
| `pandas_sample_stratified` | ğŸ² Stratified sample | Balanced sampling |
| `pandas_sample_random` | ğŸ² Random sample | Quick data preview |
| `pandas_count_distinct` | ğŸ”¢ Count unique values | Cardinality analysis |
| `pandas_export` | ğŸ’¾ Export to CSV/Parquet/Excel | Save results |
| `dataset_head_tail` | ğŸ“„ Get first/last N rows | Quick data preview |

---

### âš¡ Polars Tools (3 tools)

Lightning-fast data processing with Polars.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `polars_from_rows` | ğŸ“Š Create DataFrame | Build datasets |
| `polars_groupby` | ğŸ”¢ Group + aggregate | Fast aggregations |
| `polars_export` | ğŸ’¾ Export to CSV/Parquet/JSON | Save results |

---

### ğŸ¦† DuckDB Tools (1 tool)

SQL on local data sources. No server needed.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `duckdb_query_local` | ğŸ” SQL on local data | Query Parquet/CSV files |

---

### ğŸ¨ JSON & Data Quality (5 tools)

Transform, validate, and correlate events.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `jq_transform` | ğŸ”„ JMESPath transform | Extract/transform JSON |
| `event_validate` | âœ… JSONSchema validation | Validate data structure |
| `event_fingerprint` | ğŸ”‘ Generate fingerprint | Create unique IDs |
| `dedupe_by_id` | ğŸ§¹ Deduplicate records | Remove duplicates |
| `event_correlate` | ğŸ”— Correlate events | Join across batches |

---

### ğŸ“ File & Format Tools (4 tools)

Work with files and convert formats.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `parquet_inspect` | ğŸ” Inspect Parquet schema | Understand file structure |
| `arrow_convert` | ğŸ”„ Convert formats | Parquet â†” CSV â†” JSON |
| `fs_read` | ğŸ“– Read file (with offset) | Read large files efficiently |
| `fs_list_dir` | ğŸ“‚ List directory | Browse filesystem |
| `rg_search` | ğŸ” Search with ripgrep | Find patterns in code/data |

---

### ğŸ“Š Visualization & Evidence (2 tools)

Generate charts and evidence bundles automatically.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `dataset_to_chart` | ğŸ“ˆ Auto-generate charts | Visualize data instantly |
| `evidence_bundle_plus` | ğŸ“¦ Generate evidence bundle | Export stats + samples |

---

### ğŸ—ï¸ Infrastructure Tools (8 tools)

Monitor Nomad jobs and Consul services.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `nomad_list_jobs` | ğŸ“‹ List Nomad jobs (fuzzy search) | Find running services |
| `nomad_get_job_status` | ğŸ“Š Get full job status | Inspect job details |
| `nomad_get_allocation_logs` | ğŸ“œ Read allocation logs | Debug failing jobs |
| `nomad_get_allocation_events` | ğŸ“… Get lifecycle events | Track job history |
| `nomad_get_node_status` | ğŸ–¥ï¸ Get node status | Check node health |
| `consul_get_service_ips` | ğŸŒ Get service IPs (fuzzy) | Find service endpoints |
| `consul_list_services` | ğŸ“š List all services | Discover service catalog |
| `consul_get_service_health` | â¤ï¸ Get service health | Check service status |

**Fuzzy search**: Find services even with partial names! ğŸ¯

---

### ğŸŒ HTTP Tools (1 tool)

Make HTTP requests safely.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `http_request` | ğŸŒ HTTP GET/POST | Call APIs, test endpoints |

**Safety**: POST disabled by default. Opt-in required. âœ…

---

### âœ… Great Expectations (1 tool)

Data quality validation with GE.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `great_expectations_check` | âœ… Run GE validation | Validate data quality |

---

### ğŸ”„ Reconciliation (1 tool)

Compare datasets and find differences.

| Tool | What it does | When to use |
|------|-------------|-------------|
| `reconcile_counts` | ğŸ” Reconcile record counts | Compare datasets |

---

## ğŸ”’ Security & Guardrails

### ğŸ›¡ï¸ Built-in Safety

- **Filesystem Allowlist**: All file operations restricted to `MCPKIT_ROOTS`
- **SQL Validation**: JDBC and Athena queries validated (read-only by default)
- **HTTP Safety**: POST requests disabled by default
- **Output Limits**: Automatic caps on rows, records, and bytes
- **Path Traversal Protection**: Blocks `..` and unsafe paths

### ğŸ“ Configurable Limits

| Variable | Default | What it does |
|----------|---------|--------------|
| `MCPKIT_TIMEOUT_SECS` | `15` | Operation timeout |
| `MCPKIT_MAX_OUTPUT_BYTES` | `1000000` | Max output size |
| `MCPKIT_MAX_ROWS` | `500` | Max rows returned |
| `MCPKIT_MAX_RECORDS` | `500` | Max records returned |

---

## âš™ï¸ Configuration

### ğŸ” AWS Credentials

AWS uses the standard boto3 credential chain:

1. **Environment Variables** (recommended):
   ```bash
   export AWS_ACCESS_KEY_ID=your-key
   export AWS_SECRET_ACCESS_KEY=your-secret
   export AWS_REGION=eu-central-1
   ```

2. **AWS Credentials File** (`~/.aws/credentials`):
   ```ini
   [default]
   aws_access_key_id = your-key
   aws_secret_access_key = your-secret
   ```

3. **IAM Roles** (auto-detected on EC2/ECS/Lambda)

4. **AWS SSO** (use `aws sso login` first)

### ğŸ“ Filesystem Security

```bash
export MCPKIT_ROOTS="/allowed/path1:/allowed/path2"
```

### ğŸ§ Kafka Configuration

```bash
export MCPKIT_KAFKA_BOOTSTRAP="localhost:9092"
export MCPKIT_KAFKA_SECURITY_PROTOCOL="SASL_SSL"  # optional
export MCPKIT_KAFKA_SASL_MECHANISM="PLAIN"  # optional
export MCPKIT_KAFKA_SASL_USERNAME="user"  # optional
export MCPKIT_KAFKA_SASL_PASSWORD="pass"  # optional
```

### ğŸ“‹ Schema Registry

```bash
export MCPKIT_SCHEMA_REGISTRY_URL="http://localhost:8081"
export MCPKIT_SCHEMA_REGISTRY_BASIC_AUTH="user:pass"  # optional
```

### ğŸ—„ï¸ JDBC

```bash
export MCPKIT_JDBC_DRIVER_CLASS="org.postgresql.Driver"
export MCPKIT_JDBC_URL="jdbc:postgresql://localhost/test"
export MCPKIT_JDBC_JARS="/path/to/postgresql.jar"
export MCPKIT_JDBC_USER="user"  # optional
export MCPKIT_JDBC_PASSWORD="pass"  # optional
```

---

## ğŸ¯ Example Workflows

### ğŸ” Debug a Kafka Topic

```
You: "Check the last 10 messages from topic 'user-events'"
AI: [Uses kafka_consume_tail â†’ dataset_head_tail â†’ shows results]
```

### ğŸ“Š Analyze Database Data

```
You: "Query the users table and show me a chart of signups by month"
AI: [Uses jdbc_query_ro â†’ pandas_groupby â†’ dataset_to_chart]
```

### â˜ï¸ Query Athena and Export

```
You: "Query Athena for sales data, group by region, and export to CSV"
AI: [Uses athena_start_query â†’ athena_get_results â†’ pandas_groupby â†’ pandas_export]
```

### ğŸ—ï¸ Debug a Failing Service

```
You: "Find the 'conversion-api' service and show me its logs"
AI: [Uses nomad_list_jobs â†’ nomad_get_job_status â†’ nomad_get_allocation_logs]
```

---

## ğŸ§ª Testing

```bash
pytest
```

All tests use mocks/stubs. No real connections needed! âœ…

---

## ğŸ¤ Contributing

We â¤ï¸ contributions! Here's how to help:

1. **ğŸ› Found a bug?** Open an issue
2. **ğŸ’¡ Have an idea?** Propose a new tool or feature
3. **ğŸ”§ Want to code?** Check out `TOOLS_ANALYSIS.md` for gaps
4. **ğŸ“ Docs unclear?** Improve them!

**Design Principles:**
- ğŸ¯ **KISS**: One tool, one job
- ğŸ”— **Composable**: Tools work together seamlessly
- ğŸ”’ **Safe**: Read-only by default, explicit opt-ins
- âš¡ **Fast**: Efficient operations, smart limits

---

## ğŸ“š Learn More

- [Model Context Protocol](https://modelcontextprotocol.io) - The protocol we use
- [FastMCP](https://github.com/jlowin/fastmcp) - The framework we build on
- [Cursor](https://cursor.sh) - The IDE that makes this shine âœ¨

---

## ğŸ“„ License

MIT License - Use it, fork it, make it better! ğŸš€

---

**Made with â¤ï¸ for data engineers who want to ship faster**

*Stop context-switching. Start shipping.* ğŸ¯
